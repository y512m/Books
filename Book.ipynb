{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4c1e55-5c42-4acb-8099-67bf02ac9b8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#  Data Description\n",
    "The dataset used in this project is the text of *“A Child’s History of England”* by Charles Dickens, obtained from [Project Gutenberg](https://www.gutenberg.org/ebooks/28885).\n",
    "\n",
    "- **Format:** Plain text (.txt)  \n",
    "- **Content:** Historical account of England suitable for children.  \n",
    "- **Purpose:** Used to demonstrate text preprocessing techniques, including extraction, cleaning, and preparing data for character-level modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52aa409-e0e2-4f6b-8fe8-da73d014c24c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e53242a-3f8a-4bf9-a679-1d632613e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180da14e-9806-4b94-8804-5faddc2aa8c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# STEP 1: Load and clean text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260ad17-0a0b-4f41-8967-330bf40e6191",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Text Cleaning\n",
    "\n",
    "**Purpose:** Extract relevant portions of the text, clean it, and save the cleaned version.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Read the file:** Open `Book.txt` with UTF-8 encoding to correctly read special characters.  \n",
    "2. **Extract between markers:**  \n",
    "   - Start marker: `*** START`  \n",
    "   - End marker: `*** END`  \n",
    "   - Only text between these markers is kept.  \n",
    "3. **Basic cleaning:**  \n",
    "   - Convert text to lowercase.  \n",
    "   - Remove all characters except letters, numbers, whitespace, and basic punctuation.  \n",
    "   - Replace multiple spaces with a single space.  \n",
    "4. **Save cleaned text:** Save the processed text to `dataclean_book.txt`.  \n",
    "5. **Confirmation:** Print a message that the text has been cleaned and saved.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21bacd67-57fc-439b-8fe9-410a770822c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaned and saved to dataclean_book.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"Book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Extract between start and end markers (if exist)\n",
    "start_token = \"*** START\"\n",
    "end_token = \"*** END\"\n",
    "start_idx = text.find(start_token)\n",
    "end_idx = text.find(end_token)\n",
    "if start_idx != -1 and end_idx != -1:\n",
    "    text = text[start_idx:end_idx]\n",
    "\n",
    "# Basic cleaning\n",
    "clean_text = text.lower()\n",
    "clean_text = re.sub(r'[^a-zA-Z0-9\\s.,;:!?\\'\"-]', ' ', clean_text)\n",
    "clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "\n",
    "with open(\"dataclean_book.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(clean_text)\n",
    "\n",
    "print(\"Text cleaned and saved to dataclean_book.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7f2b9-9bf0-4da9-944e-bf93afb03029",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# STEP 2 Dataset (Char + Word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdde2c4-31af-4c43-9743-a776e454e1dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.1 Char-level Dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7250dfe-3573-4f7b-befb-83619df264c4",
   "metadata": {},
   "source": [
    "###  Character-level Data Preparation\n",
    "\n",
    "**Purpose:** Convert cleaned text into numerical form at the character level and prepare PyTorch datasets and dataloaders for model training.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Read the cleaned text:** Load `dataclean_book.txt` as a string.  \n",
    "2. **Create vocabulary:** Extract all unique characters and count them.  \n",
    "3. **Encoding and decoding dictionaries:** Map characters to integers and vice versa.  \n",
    "4. **Helper functions:** Encode string into integer list and decode integer list back to string.  \n",
    "5. **Convert text to tensor:** Transform the entire text into a PyTorch tensor of integers.  \n",
    "6. **Split data:** Use 80% for training and 20% for validation.  \n",
    "7. **Custom dataset:** Create a PyTorch Dataset that returns sequences of a fixed block size for training.  \n",
    "8. **DataLoaders:** Prepare dataloaders to feed batches of character sequences into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af7d9563-1bf9-42a3-84eb-f0720d423d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"dataclean_book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size_char = len(chars)\n",
    "\n",
    "stoi_char = {ch: i for i, ch in enumerate(chars)}\n",
    "itos_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "def encode_char(s): return [stoi_char[c] for c in s]\n",
    "def decode_char(l): return ''.join([itos_char[i] for i in l])\n",
    "\n",
    "data_char = torch.tensor(encode_char(text), dtype=torch.long)\n",
    "\n",
    "n = int(0.8 * len(data_char))\n",
    "train_data_char = data_char[:n]\n",
    "val_data_char = data_char[n:]\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size=64):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    def __len__(self): return len(self.data) - self.block_size\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.block_size]\n",
    "        y = self.data[idx+1:idx+1+self.block_size]\n",
    "        return x, y\n",
    "\n",
    "train_loader_char = DataLoader(CharDataset(train_data_char), batch_size=32, shuffle=True)\n",
    "val_loader_char = DataLoader(CharDataset(val_data_char), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc5c40-2334-430c-9e23-d529203c881e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.2 Word-level Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f888861c-7bc6-47ae-94a5-2dce7242b338",
   "metadata": {},
   "source": [
    "### Word-level Data Preparation Documentation\n",
    "\n",
    "### 1. Purpose\n",
    "Convert cleaned text into numerical form at the **word level** and prepare PyTorch datasets and dataloaders for training and validation.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Steps\n",
    "\n",
    "1. **Tokenization:**  \n",
    "   - Split the cleaned text into individual words (tokens).  \n",
    "   - Convert all words to lowercase to maintain consistency.\n",
    "\n",
    "2. **Vocabulary creation:**  \n",
    "   - Extract all unique words from the tokens.  \n",
    "   - Count the total number of unique words (`vocab_size_word`).\n",
    "\n",
    "3. **Encoding and decoding dictionaries:**  \n",
    "   - `stoi_word`: Maps each word to a unique integer.  \n",
    "   - `itos_word`: Maps integers back to their corresponding words.\n",
    "\n",
    "4. **Helper functions:**  \n",
    "   - `encode_word`: Converts a list of word tokens to a list of integer IDs.  \n",
    "   - `decode_word`: Converts a list of integer IDs back to a string of words.\n",
    "\n",
    "5. **Convert text to tensor:**  \n",
    "   - Transform the entire list of encoded words into a PyTorch tensor for model input.\n",
    "\n",
    "6. **Split data into training and validation sets:**  \n",
    "   - Use 80% of the data for training and 20% for validation.\n",
    "\n",
    "7. **Custom word-level dataset:**  \n",
    "   - Create a PyTorch Dataset that returns sequences of a fixed block size (`block_size=16`) for training.  \n",
    "   - Each sample consists of input `x` (sequence of words) and target `y` (next sequence of words).\n",
    "\n",
    "8. **DataLoaders:**  \n",
    "   - Prepare PyTorch DataLoaders for training and validation.  \n",
    "   - Batch size is set to 32.  \n",
    "   - Training loader shuffles data, validation loader does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6827db9-5cc8-494a-aac7-888e549545bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "tokens = re.findall(r\"\\b\\w+\\b\", clean_text.lower())\n",
    "words = sorted(list(set(tokens)))\n",
    "vocab_size_word = len(words)\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "# Encoding and decoding dictionaries\n",
    "stoi_word = {w: i for i, w in enumerate(words)}\n",
    "itos_word = {i: w for i, w in enumerate(words)}\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "# Helper functions\n",
    "def encode_word(tokens): return [stoi_word[w] for w in tokens]\n",
    "def decode_word(ids): return \" \".join([itos_word[i] for i in ids])\n",
    "\n",
    "    \n",
    "########################################################\n",
    "#Convert text to tensor\n",
    "data_word = torch.tensor(encode_word(tokens), dtype=torch.long)\n",
    "\n",
    "\n",
    "########################################################\n",
    "#Split data into training and validation sets\n",
    "n = int(0.8 * len(data_word))\n",
    "train_data_word = data_word[:n]\n",
    "val_data_word = data_word[n:]\n",
    "\n",
    "\n",
    "########################################################\n",
    "# Custom word-level dataset\n",
    "class WordDataset(Dataset):\n",
    "    def __init__(self, data, block_size=16):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    def __len__(self): return len(self.data) - self.block_size\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.block_size]\n",
    "        y = self.data[idx+1:idx+1+self.block_size]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "########################################################\n",
    "#DataLoaders\n",
    "train_loader_word = DataLoader(WordDataset(train_data_word), batch_size=32, shuffle=True)\n",
    "val_loader_word = DataLoader(WordDataset(val_data_word), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48513b8-7641-4701-aa6a-190fda8cbab0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# STEP 3: Models (Char + Word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c56661-df03-4e0a-a91d-235b4da124f8",
   "metadata": {},
   "source": [
    "## LSTM Model Definition and Initialization\n",
    "\n",
    "## 1. Purpose\n",
    "Define and initialize LSTM models for **both character-level and word-level modeling**.  \n",
    "- Character-level model predicts the next character in a sequence.  \n",
    "- Word-level model predicts the next word in a sequence.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Model Architecture\n",
    "\n",
    "1. **Embedding Layer:**  \n",
    "   - Converts input indices (characters or words) into dense vector representations.  \n",
    "   - Embedding size is set to 128 by default.\n",
    "\n",
    "2. **LSTM Layer:**  \n",
    "   - Processes sequences using Long Short-Term Memory units.  \n",
    "   - Hidden size is 256 by default.  \n",
    "   - Number of stacked LSTM layers is 2.  \n",
    "   - `batch_first=True` ensures input tensors have shape `(batch_size, sequence_length, features)`.\n",
    "\n",
    "3. **Fully Connected Layer:**  \n",
    "   - Maps LSTM outputs to the vocabulary space.  \n",
    "   - Produces logits for each character or word in the vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Forward Pass\n",
    "\n",
    "- Input sequence is first embedded.  \n",
    "- LSTM processes the embedded sequence and returns output and updated hidden state.  \n",
    "- Fully connected layer transforms LSTM output to logits.  \n",
    "- Logits can be used with a loss function like `CrossEntropyLoss` for training.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Device Setup\n",
    "\n",
    "- Check if GPU is available (`cuda`).  \n",
    "- Move models to the appropriate device for faster computation.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Model Initialization\n",
    "\n",
    "- `model_char`: Character-level LSTM model initialized with character vocabulary size.  \n",
    "- `model_word`: Word-level LSTM model initialized with word vocabulary size.  \n",
    "\n",
    "**Purpose:** Ready to be trained on respective datasets.  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. Summary\n",
    "\n",
    "- Both models are printed to show their architecture and parameter details.  \n",
    "- Supports training on GPU if available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "904a9a7e-5d77-416d-a33e-c885633470c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char-level model:\n",
      " LSTMModel(\n",
      "  (embed): Embedding(37, 128)\n",
      "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=37, bias=True)\n",
      ")\n",
      "Word-level model:\n",
      " LSTMModel(\n",
      "  (embed): Embedding(2582, 128)\n",
      "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=2582, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model Architecture\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=128, hidden_size=256, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    # Forward Pass\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        logits = self.fc(out)\n",
    "        return logits, hidden\n",
    "# Device Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Model Initialization\n",
    "model_char = LSTMModel(vocab_size_char).to(device)\n",
    "model_word = LSTMModel(vocab_size_word).to(device)\n",
    "\n",
    "print(\"Char-level model:\\n\", model_char)\n",
    "print(\"Word-level model:\\n\", model_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b68d91c-b80a-48a2-8566-02bc3538f5ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# STEP 4: Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0db38e-0b14-4c4f-8a69-9e66ef8a00b8",
   "metadata": {},
   "source": [
    "## LSTM Model Training\n",
    "\n",
    "## 1. Loss Function\n",
    "- **CrossEntropyLoss** is used as the criterion.  \n",
    "- Suitable for predicting discrete classes (characters or words).  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Training Function: `train_model`\n",
    "\n",
    "**Purpose:** Train the LSTM model on given datasets and monitor training and validation loss.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Steps in Training\n",
    "\n",
    "1. **Optimizer Setup:**  \n",
    "   - Use Adam optimizer with learning rate `lr` (default 0.003).  \n",
    "   - Optimizer updates the model parameters based on computed gradients.\n",
    "\n",
    "2. **Epoch Loop:**  \n",
    "   - Repeat training for a number of epochs (default 3).  \n",
    "   - Each epoch goes through all batches in the training dataset.\n",
    "\n",
    "3. **Training Loop:**  \n",
    "   - Set model to training mode (`model.train()`).  \n",
    "   - For each batch:  \n",
    "     - Move input `x` and target `y` to the selected device (CPU or GPU).  \n",
    "     - Zero gradients in the optimizer.  \n",
    "     - Forward pass through the model to get logits.  \n",
    "     - Compute loss between predicted logits and actual targets.  \n",
    "     - Backpropagate loss (`loss.backward()`).  \n",
    "     - Update model parameters (`optimizer.step()`).  \n",
    "   - Print average loss every `print_every` steps for monitoring.\n",
    "\n",
    "4. **Validation Loop:**  \n",
    "   - Set model to evaluation mode (`model.eval()`).  \n",
    "   - Disable gradient computation with `torch.no_grad()`.  \n",
    "   - Compute validation loss over the validation dataset.  \n",
    "   - Print average validation loss after each epoch.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Key Points\n",
    "\n",
    "- **Batching:** Inputs and targets are processed in batches for efficiency.  \n",
    "- **Loss Calculation:** Flatten outputs and targets to match CrossEntropyLoss requirements.  \n",
    "- **Device Management:** Ensures computations run on GPU if available for faster training.  \n",
    "- **Monitoring:** Training and validation losses are printed to track model performance and convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f38d227-4c3b-44db-91fc-9c1cf8599d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, vocab_size, epochs=3, lr=0.003, print_every=100):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(x)\n",
    "            loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if (i+1) % print_every == 0:\n",
    "                print(f\"Epoch {epoch}, Step {i+1}, Loss: {total_loss/print_every:.4f}\")\n",
    "                total_loss = 0\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                logits, _ = model(x)\n",
    "                loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "                val_loss += loss.item()\n",
    "        print(f\"Epoch {epoch} completed. Validation Loss: {val_loss/len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2df9b2-7efe-4595-8f6c-8745929292ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# STEP 5: Train Both Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf0d077-c294-43e9-acc3-ab1162924d73",
   "metadata": {},
   "source": [
    "## Training Execution\n",
    "\n",
    "## 1. Purpose\n",
    "Train both the character-level and word-level LSTM models using the prepared datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Steps\n",
    "\n",
    "1. **Training Character-level Model:**  \n",
    "   - The character-level model is trained on sequences of characters.  \n",
    "   - Number of epochs is set to 10.  \n",
    "   - Training function monitors both training and validation loss.\n",
    "\n",
    "2. **Training Word-level Model:**  \n",
    "   - The word-level model is trained on sequences of words.  \n",
    "   - Number of epochs is set to 10.  \n",
    "   - Training function monitors both training and validation loss.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Monitoring\n",
    "\n",
    "- For both models, the console prints:  \n",
    "  - Step-wise average training loss during each epoch.  \n",
    "  - Validation loss after each epoch.  \n",
    "\n",
    "**Purpose:** Ensures that the models are learning correctly and allows monitoring for convergence and overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc4d01cd-6f22-4ac6-b385-92de058dc67b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Char-level model...\n",
      "Epoch 1, Step 100, Loss: 0.2955\n",
      "Epoch 1, Step 200, Loss: 0.2815\n",
      "Epoch 1, Step 300, Loss: 0.2776\n",
      "Epoch 1, Step 400, Loss: 0.2790\n",
      "Epoch 1, Step 500, Loss: 0.2818\n",
      "Epoch 1, Step 600, Loss: 0.2846\n",
      "Epoch 1, Step 700, Loss: 0.2866\n",
      "Epoch 1, Step 800, Loss: 0.2837\n",
      "Epoch 1, Step 900, Loss: 0.2885\n",
      "Epoch 1, Step 1000, Loss: 0.2893\n",
      "Epoch 1, Step 1100, Loss: 0.2864\n",
      "Epoch 1, Step 1200, Loss: 0.2868\n",
      "Epoch 1, Step 1300, Loss: 0.2863\n",
      "Epoch 1, Step 1400, Loss: 0.2889\n",
      "Epoch 1, Step 1500, Loss: 0.2848\n",
      "Epoch 1, Step 1600, Loss: 0.2851\n",
      "Epoch 1, Step 1700, Loss: 0.2842\n",
      "Epoch 1, Step 1800, Loss: 0.2873\n",
      "Epoch 1, Step 1900, Loss: 0.2833\n",
      "Epoch 1, Step 2000, Loss: 0.2834\n",
      "Epoch 1, Step 2100, Loss: 0.2878\n",
      "Epoch 1, Step 2200, Loss: 0.2853\n",
      "Epoch 1, Step 2300, Loss: 0.2851\n",
      "Epoch 1, Step 2400, Loss: 0.2847\n",
      "Epoch 1, Step 2500, Loss: 0.2869\n",
      "Epoch 1, Step 2600, Loss: 0.2820\n",
      "Epoch 1, Step 2700, Loss: 0.2871\n",
      "Epoch 1, Step 2800, Loss: 0.2890\n",
      "Epoch 1, Step 2900, Loss: 0.2868\n",
      "Epoch 1, Step 3000, Loss: 0.2871\n",
      "Epoch 1, Step 3100, Loss: 0.2837\n",
      "Epoch 1, Step 3200, Loss: 0.2841\n",
      "Epoch 1, Step 3300, Loss: 0.2862\n",
      "Epoch 1, Step 3400, Loss: 0.2896\n",
      "Epoch 1, Step 3500, Loss: 0.2845\n",
      "Epoch 1 completed. Validation Loss: 2.7764\n",
      "Epoch 2, Step 100, Loss: 0.2842\n",
      "Epoch 2, Step 200, Loss: 0.2780\n",
      "Epoch 2, Step 300, Loss: 0.2801\n",
      "Epoch 2, Step 400, Loss: 0.2775\n",
      "Epoch 2, Step 500, Loss: 0.2774\n",
      "Epoch 2, Step 600, Loss: 0.2743\n",
      "Epoch 2, Step 700, Loss: 0.2750\n",
      "Epoch 2, Step 800, Loss: 0.2800\n",
      "Epoch 2, Step 900, Loss: 0.2839\n",
      "Epoch 2, Step 1000, Loss: 0.2827\n",
      "Epoch 2, Step 1100, Loss: 0.2877\n",
      "Epoch 2, Step 1200, Loss: 0.2863\n",
      "Epoch 2, Step 1300, Loss: 0.2857\n",
      "Epoch 2, Step 1400, Loss: 0.2869\n",
      "Epoch 2, Step 1500, Loss: 0.2803\n",
      "Epoch 2, Step 1600, Loss: 0.2854\n",
      "Epoch 2, Step 1700, Loss: 0.2834\n",
      "Epoch 2, Step 1800, Loss: 0.2818\n",
      "Epoch 2, Step 1900, Loss: 0.2824\n",
      "Epoch 2, Step 2000, Loss: 0.2809\n",
      "Epoch 2, Step 2100, Loss: 0.2885\n",
      "Epoch 2, Step 2200, Loss: 0.2823\n",
      "Epoch 2, Step 2300, Loss: 0.2852\n",
      "Epoch 2, Step 2400, Loss: 0.2848\n",
      "Epoch 2, Step 2500, Loss: 0.2892\n",
      "Epoch 2, Step 2600, Loss: 0.2822\n",
      "Epoch 2, Step 2700, Loss: 0.2839\n",
      "Epoch 2, Step 2800, Loss: 0.2853\n",
      "Epoch 2, Step 2900, Loss: 0.2825\n",
      "Epoch 2, Step 3000, Loss: 0.2783\n",
      "Epoch 2, Step 3100, Loss: 0.2837\n",
      "Epoch 2, Step 3200, Loss: 0.2806\n",
      "Epoch 2, Step 3300, Loss: 0.2821\n",
      "Epoch 2, Step 3400, Loss: 0.2775\n",
      "Epoch 2, Step 3500, Loss: 0.2791\n",
      "Epoch 2 completed. Validation Loss: 2.8215\n",
      "Epoch 3, Step 100, Loss: 0.2822\n",
      "Epoch 3, Step 200, Loss: 0.2783\n",
      "Epoch 3, Step 300, Loss: 0.2794\n",
      "Epoch 3, Step 400, Loss: 0.2782\n",
      "Epoch 3, Step 500, Loss: 0.2760\n",
      "Epoch 3, Step 600, Loss: 0.2795\n",
      "Epoch 3, Step 700, Loss: 0.2777\n",
      "Epoch 3, Step 800, Loss: 0.2761\n",
      "Epoch 3, Step 900, Loss: 0.2821\n",
      "Epoch 3, Step 1000, Loss: 0.2794\n",
      "Epoch 3, Step 1100, Loss: 0.2768\n",
      "Epoch 3, Step 1200, Loss: 0.2792\n",
      "Epoch 3, Step 1300, Loss: 0.2807\n",
      "Epoch 3, Step 1400, Loss: 0.2838\n",
      "Epoch 3, Step 1500, Loss: 0.2784\n",
      "Epoch 3, Step 1600, Loss: 0.2815\n",
      "Epoch 3, Step 1700, Loss: 0.2820\n",
      "Epoch 3, Step 1800, Loss: 0.2876\n",
      "Epoch 3, Step 1900, Loss: 0.2850\n",
      "Epoch 3, Step 2000, Loss: 0.2881\n",
      "Epoch 3, Step 2100, Loss: 0.2828\n",
      "Epoch 3, Step 2200, Loss: 0.2840\n",
      "Epoch 3, Step 2300, Loss: 0.2875\n",
      "Epoch 3, Step 2400, Loss: 0.2869\n",
      "Epoch 3, Step 2500, Loss: 0.2855\n",
      "Epoch 3, Step 2600, Loss: 0.2902\n",
      "Epoch 3, Step 2700, Loss: 0.2831\n",
      "Epoch 3, Step 2800, Loss: 0.2837\n",
      "Epoch 3, Step 2900, Loss: 0.2824\n",
      "Epoch 3, Step 3000, Loss: 0.2801\n",
      "Epoch 3, Step 3100, Loss: 0.2802\n",
      "Epoch 3, Step 3200, Loss: 0.2820\n",
      "Epoch 3, Step 3300, Loss: 0.2801\n",
      "Epoch 3, Step 3400, Loss: 0.2830\n",
      "Epoch 3, Step 3500, Loss: 0.2807\n",
      "Epoch 3 completed. Validation Loss: 2.8441\n",
      "Epoch 4, Step 100, Loss: 0.2806\n",
      "Epoch 4, Step 200, Loss: 0.2709\n",
      "Epoch 4, Step 300, Loss: 0.2737\n",
      "Epoch 4, Step 400, Loss: 0.2719\n",
      "Epoch 4, Step 500, Loss: 0.2746\n",
      "Epoch 4, Step 600, Loss: 0.2780\n",
      "Epoch 4, Step 700, Loss: 0.2807\n",
      "Epoch 4, Step 800, Loss: 0.2815\n",
      "Epoch 4, Step 900, Loss: 0.2827\n",
      "Epoch 4, Step 1000, Loss: 0.2820\n",
      "Epoch 4, Step 1100, Loss: 0.2828\n",
      "Epoch 4, Step 1200, Loss: 0.2828\n",
      "Epoch 4, Step 1300, Loss: 0.2798\n",
      "Epoch 4, Step 1400, Loss: 0.2874\n",
      "Epoch 4, Step 1500, Loss: 0.2814\n",
      "Epoch 4, Step 1600, Loss: 0.2836\n",
      "Epoch 4, Step 1700, Loss: 0.2840\n",
      "Epoch 4, Step 1800, Loss: 0.2859\n",
      "Epoch 4, Step 1900, Loss: 0.2863\n",
      "Epoch 4, Step 2000, Loss: 0.2900\n",
      "Epoch 4, Step 2100, Loss: 0.2901\n",
      "Epoch 4, Step 2200, Loss: 0.2868\n",
      "Epoch 4, Step 2300, Loss: 0.2843\n",
      "Epoch 4, Step 2400, Loss: 0.2834\n",
      "Epoch 4, Step 2500, Loss: 0.2843\n",
      "Epoch 4, Step 2600, Loss: 0.2829\n",
      "Epoch 4, Step 2700, Loss: 0.2784\n",
      "Epoch 4, Step 2800, Loss: 0.2803\n",
      "Epoch 4, Step 2900, Loss: 0.2797\n",
      "Epoch 4, Step 3000, Loss: 0.2826\n",
      "Epoch 4, Step 3100, Loss: 0.2841\n",
      "Epoch 4, Step 3200, Loss: 0.2833\n",
      "Epoch 4, Step 3300, Loss: 0.2875\n",
      "Epoch 4, Step 3400, Loss: 0.2874\n",
      "Epoch 4, Step 3500, Loss: 0.2864\n",
      "Epoch 4 completed. Validation Loss: 2.8309\n",
      "Epoch 5, Step 100, Loss: 0.2871\n",
      "Epoch 5, Step 200, Loss: 0.2804\n",
      "Epoch 5, Step 300, Loss: 0.2727\n",
      "Epoch 5, Step 400, Loss: 0.2711\n",
      "Epoch 5, Step 500, Loss: 0.2727\n",
      "Epoch 5, Step 600, Loss: 0.2712\n",
      "Epoch 5, Step 700, Loss: 0.2786\n",
      "Epoch 5, Step 800, Loss: 0.2788\n",
      "Epoch 5, Step 900, Loss: 0.2792\n",
      "Epoch 5, Step 1000, Loss: 0.2797\n",
      "Epoch 5, Step 1100, Loss: 0.2818\n",
      "Epoch 5, Step 1200, Loss: 0.2868\n",
      "Epoch 5, Step 1300, Loss: 0.2807\n",
      "Epoch 5, Step 1400, Loss: 0.2812\n",
      "Epoch 5, Step 1500, Loss: 0.2881\n",
      "Epoch 5, Step 1600, Loss: 0.2862\n",
      "Epoch 5, Step 1700, Loss: 0.2834\n",
      "Epoch 5, Step 1800, Loss: 0.2835\n",
      "Epoch 5, Step 1900, Loss: 0.2808\n",
      "Epoch 5, Step 2000, Loss: 0.2821\n",
      "Epoch 5, Step 2100, Loss: 0.2834\n",
      "Epoch 5, Step 2200, Loss: 0.2839\n",
      "Epoch 5, Step 2300, Loss: 0.2862\n",
      "Epoch 5, Step 2400, Loss: 0.2830\n",
      "Epoch 5, Step 2500, Loss: 0.2839\n",
      "Epoch 5, Step 2600, Loss: 0.2826\n",
      "Epoch 5, Step 2700, Loss: 0.2831\n",
      "Epoch 5, Step 2800, Loss: 0.2868\n",
      "Epoch 5, Step 2900, Loss: 0.2834\n",
      "Epoch 5, Step 3000, Loss: 0.2868\n",
      "Epoch 5, Step 3100, Loss: 0.2877\n",
      "Epoch 5, Step 3200, Loss: 0.2899\n",
      "Epoch 5, Step 3300, Loss: 0.2927\n",
      "Epoch 5, Step 3400, Loss: 0.2907\n",
      "Epoch 5, Step 3500, Loss: 0.2919\n",
      "Epoch 5 completed. Validation Loss: 2.8410\n",
      "Epoch 6, Step 100, Loss: 0.2939\n",
      "Epoch 6, Step 200, Loss: 0.2950\n",
      "Epoch 6, Step 300, Loss: 0.2867\n",
      "Epoch 6, Step 400, Loss: 0.2860\n",
      "Epoch 6, Step 500, Loss: 0.2815\n",
      "Epoch 6, Step 600, Loss: 0.2793\n",
      "Epoch 6, Step 700, Loss: 0.2790\n",
      "Epoch 6, Step 800, Loss: 0.2820\n",
      "Epoch 6, Step 900, Loss: 0.2786\n",
      "Epoch 6, Step 1000, Loss: 0.2821\n",
      "Epoch 6, Step 1100, Loss: 0.2768\n",
      "Epoch 6, Step 1200, Loss: 0.2778\n",
      "Epoch 6, Step 1300, Loss: 0.2827\n",
      "Epoch 6, Step 1400, Loss: 0.2845\n",
      "Epoch 6, Step 1500, Loss: 0.2811\n",
      "Epoch 6, Step 1600, Loss: 0.2820\n",
      "Epoch 6, Step 1700, Loss: 0.2843\n",
      "Epoch 6, Step 1800, Loss: 0.2875\n",
      "Epoch 6, Step 1900, Loss: 0.2899\n",
      "Epoch 6, Step 2000, Loss: 0.2906\n",
      "Epoch 6, Step 2100, Loss: 0.2900\n",
      "Epoch 6, Step 2200, Loss: 0.2916\n",
      "Epoch 6, Step 2300, Loss: 0.2882\n",
      "Epoch 6, Step 2400, Loss: 0.2888\n",
      "Epoch 6, Step 2500, Loss: 0.2917\n",
      "Epoch 6, Step 2600, Loss: 0.2857\n",
      "Epoch 6, Step 2700, Loss: 0.2860\n",
      "Epoch 6, Step 2800, Loss: 0.2863\n",
      "Epoch 6, Step 2900, Loss: 0.2860\n",
      "Epoch 6, Step 3000, Loss: 0.2853\n",
      "Epoch 6, Step 3100, Loss: 0.2821\n",
      "Epoch 6, Step 3200, Loss: 0.2791\n",
      "Epoch 6, Step 3300, Loss: 0.2826\n",
      "Epoch 6, Step 3400, Loss: 0.2870\n",
      "Epoch 6, Step 3500, Loss: 0.2866\n",
      "Epoch 6 completed. Validation Loss: 2.8235\n",
      "Epoch 7, Step 100, Loss: 0.2864\n",
      "Epoch 7, Step 200, Loss: 0.2842\n",
      "Epoch 7, Step 300, Loss: 0.2824\n",
      "Epoch 7, Step 400, Loss: 0.2815\n",
      "Epoch 7, Step 500, Loss: 0.2782\n",
      "Epoch 7, Step 600, Loss: 0.2833\n",
      "Epoch 7, Step 700, Loss: 0.2931\n",
      "Epoch 7, Step 800, Loss: 0.2935\n",
      "Epoch 7, Step 900, Loss: 0.2911\n",
      "Epoch 7, Step 1000, Loss: 0.2889\n",
      "Epoch 7, Step 1100, Loss: 0.2907\n",
      "Epoch 7, Step 1200, Loss: 0.2880\n",
      "Epoch 7, Step 1300, Loss: 0.2872\n",
      "Epoch 7, Step 1400, Loss: 0.2917\n",
      "Epoch 7, Step 1500, Loss: 0.2875\n",
      "Epoch 7, Step 1600, Loss: 0.2882\n",
      "Epoch 7, Step 1700, Loss: 0.2868\n",
      "Epoch 7, Step 1800, Loss: 0.2869\n",
      "Epoch 7, Step 1900, Loss: 0.2911\n",
      "Epoch 7, Step 2000, Loss: 0.2905\n",
      "Epoch 7, Step 2100, Loss: 0.2878\n",
      "Epoch 7, Step 2200, Loss: 0.2852\n",
      "Epoch 7, Step 2300, Loss: 0.2887\n",
      "Epoch 7, Step 2400, Loss: 0.2850\n",
      "Epoch 7, Step 2500, Loss: 0.2866\n",
      "Epoch 7, Step 2600, Loss: 0.2901\n",
      "Epoch 7, Step 2700, Loss: 0.2919\n",
      "Epoch 7, Step 2800, Loss: 0.2944\n",
      "Epoch 7, Step 2900, Loss: 0.2947\n",
      "Epoch 7, Step 3000, Loss: 0.2934\n",
      "Epoch 7, Step 3100, Loss: 0.2910\n",
      "Epoch 7, Step 3200, Loss: 0.2876\n",
      "Epoch 7, Step 3300, Loss: 0.2872\n",
      "Epoch 7, Step 3400, Loss: 0.2837\n",
      "Epoch 7, Step 3500, Loss: 0.2880\n",
      "Epoch 7 completed. Validation Loss: 2.8657\n",
      "Epoch 8, Step 100, Loss: 0.2890\n",
      "Epoch 8, Step 200, Loss: 0.2881\n",
      "Epoch 8, Step 300, Loss: 0.2868\n",
      "Epoch 8, Step 400, Loss: 0.2873\n",
      "Epoch 8, Step 500, Loss: 0.2855\n",
      "Epoch 8, Step 600, Loss: 0.2872\n",
      "Epoch 8, Step 700, Loss: 0.2863\n",
      "Epoch 8, Step 800, Loss: 0.2845\n",
      "Epoch 8, Step 900, Loss: 0.2878\n",
      "Epoch 8, Step 1000, Loss: 0.2888\n",
      "Epoch 8, Step 1100, Loss: 0.2900\n",
      "Epoch 8, Step 1200, Loss: 0.2929\n",
      "Epoch 8, Step 1300, Loss: 0.2925\n",
      "Epoch 8, Step 1400, Loss: 0.2941\n",
      "Epoch 8, Step 1500, Loss: 0.2942\n",
      "Epoch 8, Step 1600, Loss: 0.2925\n",
      "Epoch 8, Step 1700, Loss: 0.2909\n",
      "Epoch 8, Step 1800, Loss: 0.2889\n",
      "Epoch 8, Step 1900, Loss: 0.2918\n",
      "Epoch 8, Step 2000, Loss: 0.2929\n",
      "Epoch 8, Step 2100, Loss: 0.2927\n",
      "Epoch 8, Step 2200, Loss: 0.2901\n",
      "Epoch 8, Step 2300, Loss: 0.2909\n",
      "Epoch 8, Step 2400, Loss: 0.2888\n",
      "Epoch 8, Step 2500, Loss: 0.2933\n",
      "Epoch 8, Step 2600, Loss: 0.2974\n",
      "Epoch 8, Step 2700, Loss: 0.2971\n",
      "Epoch 8, Step 2800, Loss: 0.2980\n",
      "Epoch 8, Step 2900, Loss: 0.2921\n",
      "Epoch 8, Step 3000, Loss: 0.2931\n",
      "Epoch 8, Step 3100, Loss: 0.3009\n",
      "Epoch 8, Step 3200, Loss: 0.2989\n",
      "Epoch 8, Step 3300, Loss: 0.2982\n",
      "Epoch 8, Step 3400, Loss: 0.2949\n",
      "Epoch 8, Step 3500, Loss: 0.2955\n",
      "Epoch 8 completed. Validation Loss: 2.8627\n",
      "Epoch 9, Step 100, Loss: 0.2914\n",
      "Epoch 9, Step 200, Loss: 0.2909\n",
      "Epoch 9, Step 300, Loss: 0.2874\n",
      "Epoch 9, Step 400, Loss: 0.2891\n",
      "Epoch 9, Step 500, Loss: 0.2885\n",
      "Epoch 9, Step 600, Loss: 0.2868\n",
      "Epoch 9, Step 700, Loss: 0.2866\n",
      "Epoch 9, Step 800, Loss: 0.2909\n",
      "Epoch 9, Step 900, Loss: 0.2891\n",
      "Epoch 9, Step 1000, Loss: 0.2874\n",
      "Epoch 9, Step 1100, Loss: 0.2915\n",
      "Epoch 9, Step 1200, Loss: 0.2926\n",
      "Epoch 9, Step 1300, Loss: 0.2958\n",
      "Epoch 9, Step 1400, Loss: 0.2934\n",
      "Epoch 9, Step 1500, Loss: 0.2934\n",
      "Epoch 9, Step 1600, Loss: 0.2933\n",
      "Epoch 9, Step 1700, Loss: 0.2940\n",
      "Epoch 9, Step 1800, Loss: 0.2960\n",
      "Epoch 9, Step 1900, Loss: 0.2984\n",
      "Epoch 9, Step 2000, Loss: 0.2965\n",
      "Epoch 9, Step 2100, Loss: 0.2982\n",
      "Epoch 9, Step 2200, Loss: 0.2938\n",
      "Epoch 9, Step 2300, Loss: 0.2960\n",
      "Epoch 9, Step 2400, Loss: 0.2966\n",
      "Epoch 9, Step 2500, Loss: 0.2999\n",
      "Epoch 9, Step 2600, Loss: 0.3002\n",
      "Epoch 9, Step 2700, Loss: 0.3013\n",
      "Epoch 9, Step 2800, Loss: 0.3041\n",
      "Epoch 9, Step 2900, Loss: 0.3040\n",
      "Epoch 9, Step 3000, Loss: 0.3005\n",
      "Epoch 9, Step 3100, Loss: 0.3012\n",
      "Epoch 9, Step 3200, Loss: 0.3043\n",
      "Epoch 9, Step 3300, Loss: 0.3000\n",
      "Epoch 9, Step 3400, Loss: 0.3004\n",
      "Epoch 9, Step 3500, Loss: 0.2987\n",
      "Epoch 9 completed. Validation Loss: 2.8371\n",
      "Epoch 10, Step 100, Loss: 0.3023\n",
      "Epoch 10, Step 200, Loss: 0.2952\n",
      "Epoch 10, Step 300, Loss: 0.2962\n",
      "Epoch 10, Step 400, Loss: 0.2961\n",
      "Epoch 10, Step 500, Loss: 0.2920\n",
      "Epoch 10, Step 600, Loss: 0.2953\n",
      "Epoch 10, Step 700, Loss: 0.2897\n",
      "Epoch 10, Step 800, Loss: 0.2880\n",
      "Epoch 10, Step 900, Loss: 0.2933\n",
      "Epoch 10, Step 1000, Loss: 0.2909\n",
      "Epoch 10, Step 1100, Loss: 0.2884\n",
      "Epoch 10, Step 1200, Loss: 0.2896\n",
      "Epoch 10, Step 1300, Loss: 0.2916\n",
      "Epoch 10, Step 1400, Loss: 0.2918\n",
      "Epoch 10, Step 1500, Loss: 0.2947\n",
      "Epoch 10, Step 1600, Loss: 0.2974\n",
      "Epoch 10, Step 1700, Loss: 0.3037\n",
      "Epoch 10, Step 1800, Loss: 0.3074\n",
      "Epoch 10, Step 1900, Loss: 0.3070\n",
      "Epoch 10, Step 2000, Loss: 0.3104\n",
      "Epoch 10, Step 2100, Loss: 0.3072\n",
      "Epoch 10, Step 2200, Loss: 0.3012\n",
      "Epoch 10, Step 2300, Loss: 0.3046\n",
      "Epoch 10, Step 2400, Loss: 0.3096\n",
      "Epoch 10, Step 2500, Loss: 0.3146\n",
      "Epoch 10, Step 2600, Loss: 0.3113\n",
      "Epoch 10, Step 2700, Loss: 0.3072\n",
      "Epoch 10, Step 2800, Loss: 0.3043\n",
      "Epoch 10, Step 2900, Loss: 0.3004\n",
      "Epoch 10, Step 3000, Loss: 0.3037\n",
      "Epoch 10, Step 3100, Loss: 0.3053\n",
      "Epoch 10, Step 3200, Loss: 0.3033\n",
      "Epoch 10, Step 3300, Loss: 0.3025\n",
      "Epoch 10, Step 3400, Loss: 0.3050\n",
      "Epoch 10, Step 3500, Loss: 0.3059\n",
      "Epoch 10 completed. Validation Loss: 2.8401\n",
      "\n",
      "Training Word-level model...\n",
      "Epoch 1, Step 100, Loss: 0.5108\n",
      "Epoch 1, Step 200, Loss: 0.5052\n",
      "Epoch 1, Step 300, Loss: 0.4974\n",
      "Epoch 1, Step 400, Loss: 0.5043\n",
      "Epoch 1, Step 500, Loss: 0.5066\n",
      "Epoch 1, Step 600, Loss: 0.5002\n",
      "Epoch 1 completed. Validation Loss: 10.2856\n",
      "Epoch 2, Step 100, Loss: 0.4052\n",
      "Epoch 2, Step 200, Loss: 0.4237\n",
      "Epoch 2, Step 300, Loss: 0.4348\n",
      "Epoch 2, Step 400, Loss: 0.4464\n",
      "Epoch 2, Step 500, Loss: 0.4469\n",
      "Epoch 2, Step 600, Loss: 0.4499\n",
      "Epoch 2 completed. Validation Loss: 10.8683\n",
      "Epoch 3, Step 100, Loss: 0.3889\n",
      "Epoch 3, Step 200, Loss: 0.3944\n",
      "Epoch 3, Step 300, Loss: 0.4051\n",
      "Epoch 3, Step 400, Loss: 0.4197\n",
      "Epoch 3, Step 500, Loss: 0.4271\n",
      "Epoch 3, Step 600, Loss: 0.4296\n",
      "Epoch 3 completed. Validation Loss: 11.2108\n",
      "Epoch 4, Step 100, Loss: 0.3737\n",
      "Epoch 4, Step 200, Loss: 0.3784\n",
      "Epoch 4, Step 300, Loss: 0.3932\n",
      "Epoch 4, Step 400, Loss: 0.3985\n",
      "Epoch 4, Step 500, Loss: 0.4075\n",
      "Epoch 4, Step 600, Loss: 0.4134\n",
      "Epoch 4 completed. Validation Loss: 11.6038\n",
      "Epoch 5, Step 100, Loss: 0.3567\n",
      "Epoch 5, Step 200, Loss: 0.3679\n",
      "Epoch 5, Step 300, Loss: 0.3865\n",
      "Epoch 5, Step 400, Loss: 0.3929\n",
      "Epoch 5, Step 500, Loss: 0.4019\n",
      "Epoch 5, Step 600, Loss: 0.4015\n",
      "Epoch 5 completed. Validation Loss: 11.9160\n",
      "Epoch 6, Step 100, Loss: 0.3527\n",
      "Epoch 6, Step 200, Loss: 0.3645\n",
      "Epoch 6, Step 300, Loss: 0.3727\n",
      "Epoch 6, Step 400, Loss: 0.3782\n",
      "Epoch 6, Step 500, Loss: 0.3888\n",
      "Epoch 6, Step 600, Loss: 0.3921\n",
      "Epoch 6 completed. Validation Loss: 12.0907\n",
      "Epoch 7, Step 100, Loss: 0.3466\n",
      "Epoch 7, Step 200, Loss: 0.3595\n",
      "Epoch 7, Step 300, Loss: 0.3644\n",
      "Epoch 7, Step 400, Loss: 0.3760\n",
      "Epoch 7, Step 500, Loss: 0.3842\n",
      "Epoch 7, Step 600, Loss: 0.3887\n",
      "Epoch 7 completed. Validation Loss: 12.4412\n",
      "Epoch 8, Step 100, Loss: 0.3402\n",
      "Epoch 8, Step 200, Loss: 0.3557\n",
      "Epoch 8, Step 300, Loss: 0.3621\n",
      "Epoch 8, Step 400, Loss: 0.3711\n",
      "Epoch 8, Step 500, Loss: 0.3785\n",
      "Epoch 8, Step 600, Loss: 0.3900\n",
      "Epoch 8 completed. Validation Loss: 12.4996\n",
      "Epoch 9, Step 100, Loss: 0.3372\n",
      "Epoch 9, Step 200, Loss: 0.3503\n",
      "Epoch 9, Step 300, Loss: 0.3612\n",
      "Epoch 9, Step 400, Loss: 0.3713\n",
      "Epoch 9, Step 500, Loss: 0.3717\n",
      "Epoch 9, Step 600, Loss: 0.3785\n",
      "Epoch 9 completed. Validation Loss: 12.7294\n",
      "Epoch 10, Step 100, Loss: 0.3346\n",
      "Epoch 10, Step 200, Loss: 0.3477\n",
      "Epoch 10, Step 300, Loss: 0.3588\n",
      "Epoch 10, Step 400, Loss: 0.3628\n",
      "Epoch 10, Step 500, Loss: 0.3686\n",
      "Epoch 10, Step 600, Loss: 0.3772\n",
      "Epoch 10 completed. Validation Loss: 12.7919\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Char-level model...\")\n",
    "train_model(model_char, train_loader_char, val_loader_char, vocab_size_char, epochs=10)\n",
    "\n",
    "print(\"\\nTraining Word-level model...\")\n",
    "train_model(model_word, train_loader_word, val_loader_word, vocab_size_word, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca05138-e307-44b4-90f1-9da086aae6d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# STEP 6: Evaluation & Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb2307-fcfc-416e-a28c-8509e863c2f9",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "## 1. Purpose\n",
    "Evaluate the performance of both character-level and word-level LSTM models on the validation datasets.  \n",
    "- Compute **validation loss** to measure how well the model predicts unseen data.  \n",
    "- Compute **perplexity** to quantify prediction uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Steps\n",
    "\n",
    "1. **Set model to evaluation mode:**  \n",
    "   - Disable dropout and other training-specific layers using `model.eval()`.  \n",
    "   - Disable gradient computation with `torch.no_grad()` for efficiency.\n",
    "\n",
    "2. **Iterate over validation data:**  \n",
    "   - For each batch, move input `x` and target `y` to the selected device (CPU or GPU).  \n",
    "   - Forward pass through the model to get logits.  \n",
    "   - Compute loss using `CrossEntropyLoss`.  \n",
    "   - Accumulate total loss across all batches.\n",
    "\n",
    "3. **Compute average loss and perplexity:**  \n",
    "   - Average loss = total loss divided by number of batches.  \n",
    "   - Perplexity = `exp(average loss)`; lower perplexity indicates better model performance.\n",
    "\n",
    "4. **Compare models:**  \n",
    "   - Character-level and word-level validation losses and perplexities are printed.  \n",
    "   - The model with lower perplexity is considered to perform better.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Output\n",
    "\n",
    "- **Char-level model:** Validation loss and perplexity.  \n",
    "- **Word-level model:** Validation loss and perplexity.  \n",
    "- **Comparison statement:** Indicates which model performs better on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97d6a24f-733c-48a2-b3ef-3b5c3453ff96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Char-level] Validation Loss: 2.8401, Perplexity: 17.12\n",
      "[Word-level] Validation Loss: 12.7919, Perplexity: 359299.56\n",
      " Char-level performs better.\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, data_loader, vocab_size):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, _ = model(x)\n",
    "            loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    ppl = math.exp(avg_loss)\n",
    "    return avg_loss, ppl\n",
    "\n",
    "char_val_loss, char_val_ppl = evaluate(model_char, val_loader_char, vocab_size_char)\n",
    "word_val_loss, word_val_ppl = evaluate(model_word, val_loader_word, vocab_size_word)\n",
    "\n",
    "print(f\"[Char-level] Validation Loss: {char_val_loss:.4f}, Perplexity: {char_val_ppl:.2f}\")\n",
    "print(f\"[Word-level] Validation Loss: {word_val_loss:.4f}, Perplexity: {word_val_ppl:.2f}\")\n",
    "\n",
    "if word_val_ppl < char_val_ppl:\n",
    "    print(\" Word-level performs better.\")\n",
    "else:\n",
    "    print(\" Char-level performs better.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ac95e-da51-4e84-b669-213955434307",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# STEP 7: Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a0f3e-f65f-49fd-acc2-10cea5d173c6",
   "metadata": {},
   "source": [
    "## Text Generation with Trained LSTM Models\n",
    "\n",
    "## 1. Purpose\n",
    "Generate new text sequences using the trained **character-level** and **word-level** LSTM models.  \n",
    "- Character-level model predicts one character at a time.  \n",
    "- Word-level model predicts one word at a time.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Generation Process\n",
    "\n",
    "1. **Set model to evaluation mode:**  \n",
    "   - Disable dropout and other training-specific layers with `model.eval()`.  \n",
    "   - Use `torch.no_grad()` to prevent gradient computation.\n",
    "\n",
    "2. **Prepare initial input (start text):**  \n",
    "   - Convert starting characters or words to integer IDs using the respective encoding dictionaries (`stoi_char` or `stoi_word`).  \n",
    "   - Initialize the hidden state of the LSTM to `None`.\n",
    "\n",
    "3. **Iterative generation loop:**  \n",
    "   - For a specified sequence length:  \n",
    "     - Forward pass through the model to get logits for the next character or word.  \n",
    "     - Apply temperature scaling to control randomness.  \n",
    "     - Convert logits to probabilities using softmax.  \n",
    "     - Sample the next character or word from the probability distribution.  \n",
    "     - Append the generated character or word to the sequence.  \n",
    "     - Update input IDs for the next step.\n",
    "\n",
    "4. **Return generated text:**  \n",
    "   - Character-level: Join generated characters into a string.  \n",
    "   - Word-level: Join generated words into a string.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Parameters\n",
    "\n",
    "- **start_text:** Initial string to seed generation (e.g., `\"the \"` or `\"the cat\"`).  \n",
    "- **length:** Number of characters or words to generate.  \n",
    "- **temperature:** Controls randomness; higher values produce more diverse outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Output\n",
    "\n",
    "- Prints a sample of generated text for:  \n",
    "  - **Character-level model** (e.g., 200 characters)  \n",
    "  - **Word-level model** (e.g., 20 words)  \n",
    "- Demonstrates the model’s ability to generate coherent sequences resembling the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c2e718c-1ed5-4779-8049-0bac26bc32e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char-level sample:\n",
      " the ratched a couple knew that it was over at last: and i do see you re trying to me dear, and a bright to-days and camouse go on one, said alice. right, was in the distance, sitting sad and lonely on a l\n",
      "\n",
      "Word-level sample:\n",
      " the cat or you are you fond of of dogs the mouse did not answer so alice went on eagerly there is\n"
     ]
    }
   ],
   "source": [
    "def generate_char(model, start_text=\"the \", length=200, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_ids = torch.tensor([stoi_char[c] for c in start_text], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = None\n",
    "    generated = list(start_text)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            logits, hidden = model(input_ids, hidden)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, 1).item()\n",
    "            generated.append(itos_char[next_id])\n",
    "            input_ids = torch.tensor([[next_id]], dtype=torch.long).to(device)\n",
    "    return \"\".join(generated)\n",
    "\n",
    "def generate_word(model, start_text=\"the cat\", length=20, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_ids = torch.tensor([stoi_word[w] for w in start_text.split() if w in stoi_word],\n",
    "                             dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = None\n",
    "    generated = start_text.split()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            logits, hidden = model(input_ids, hidden)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, 1).item()\n",
    "            generated.append(itos_word[next_id])\n",
    "            input_ids = torch.tensor([[next_id]], dtype=torch.long).to(device)\n",
    "    return \" \".join(generated)\n",
    "\n",
    "print(\"Char-level sample:\\n\", generate_char(model_char, start_text=\"the \", length=200)) # you can change the start_text\n",
    "print(\"\\nWord-level sample:\\n\", generate_word(model_word, start_text=\"the cat\", length=20)) # you can change the start_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac9e94-a8fd-4a76-9ba7-95e890e8f0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
